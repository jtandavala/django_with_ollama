Hereâ€™s an improved version of your project instructions, providing more clarity and detail:

---

# **OLLAMA WITH DJANGO**

This project integrates Ollama with Django, enabling AI-driven predictions via a REST API.

## **Getting Started**

Follow the steps below to set up and run the project:

### **1. Prerequisites**

- Ensure you have **Ollama** installed on your machine.
- Install **UV** for project management by following the [UV documentation](https://docs.astral.sh/uv/).

### **2. Install Dependencies**

1. Clone this repository to your local machine.
2. Navigate to the project directory:
   ```bash
   cd <project-directory>
   ```
3. Sync the project dependencies using **UV**:
   ```bash
   uv sync
   ```

### **3. Configure Ollama**

1. Pull a large language model (LLM) of your choice from Ollama:
   ```bash
   ollama pull <model-name>
   ```
   Replace `<model-name>` with the desired LLM, e.g., `phi3`.

2. Start the Ollama service:
   ```bash
   ollama serve
   ```

### **4. Run the Django Project**

Start the Django development server:
```bash
uv run python manage.py runserver
```

The server will run on `http://127.0.0.1:8000` by default.

### **5. Test the API**

You can test the `/api/predict/` endpoint using `curl`:

```bash
curl -X POST \
     -H "Content-Type: application/json" \
     -d '{"input": "who are you"}' \
     http://127.0.0.1:8000/api/predict/
```

### **Expected Response**
The response will contain the output generated by the LLM. For example:
```json
{
    "response": "I am a helpful AI assistant."
}
```

---

## **Project Structure**

The project includes:
- **Django Backend**: Handles API requests and integrates with the Ollama LLM.
- **OLLAMA API**: Serves the LLM responses.

---

## **Additional Notes**

- Ensure that **Ollama** is running and the selected LLM is properly loaded before making requests to the API.
- If you encounter issues, ensure that the `OLLAMA_API_URL` environment variable is correctly configured (default: `http://localhost:11434`).

---

Let me know if you'd like further refinements!